# Manuscript Verification System: Design Document

**Project**: Distance Ladder Systematics (v8.6H)
**Author**: Aaron Wiley
**Date**: 2025-11-18
**Status**: Implementation Phase
**Purpose**: Case study in scientific software quality assurance for publication

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Problem Statement](#problem-statement)
3. [Incident Analysis](#incident-analysis)
4. [Requirements Analysis](#requirements-analysis)
5. [Architecture Design](#architecture-design)
6. [Implementation Plan](#implementation-plan)
7. [Verification Checks Specification](#verification-checks-specification)
8. [Future Evolution Path](#future-evolution-path)

---

## Executive Summary

**The Problem**: A publication-ready astrophysics manuscript with ~50 numerical claims scattered across 15 data files, 8 figures, 8 tables, README, and manuscript text experienced multiple data consistency incidents during final polishing.

**The Solution**: A lightweight verification framework that validates critical numerical invariants across the entire manuscript pipeline, catching 80% of inconsistency issues with <5 hours of development effort.

**Key Innovation**: Treating manuscript consistency as a software testing problem, applying contract-driven design principles to scientific publishing workflows.

**Target Audience**: Computational scientists, research software engineers, and reproducibility advocates.

---

## Problem Statement

### Context

The `distance-ladder-systematics` repository contains a comprehensive analysis of the Hubble tension, reducing it from 5.9σ to 1.1σ through systematic error reassessment. The repository includes:

- **15 CSV data files** with interconnected numerical results
- **8 publication-quality figures** generated programmatically from data
- **8 LaTeX tables** auto-generated from CSV sources
- **1 manuscript** (LaTeX) with ~50 numerical assertions
- **1 README** with key results summary table
- **Multiple analysis scripts** (Python) that generate derived values

### The Consistency Challenge

**Dependency Graph Complexity:**

```
systematic_error_budget.csv
    ├─> create_figure2_error_budget.py → figure2_error_budget.png
    ├─> create_manuscript_tables.py → table1_systematic_budget.tex
    ├─> README.md (manual updates)
    └─> manuscript.tex §4.2 (manual references)

tension_evolution.csv
    ├─> create_figure1_tension_evolution.py → figure1_tension_evolution.png
    ├─> create_manuscript_tables.py → table2_tension_evolution.tex
    ├─> README.md (manual tension table)
    └─> manuscript.tex §5.1 (manual references)

h0_measurements_compilation.csv
    ├─> create_figure4_h0_compilation.py → figure4_h0_compilation.png
    ├─> create_manuscript_tables.py → table3_h0_compilation.tex
    ├─> README.md (manual convergence values)
    └─> manuscript.tex §5.2 (manual references)

[...and 12 more similar chains]
```

**The Core Issue**: No single source of truth, with both automated (figures/tables) and manual (README/manuscript) consumption of the same data. When values update, consistency failures cascade.

---

## Incident Analysis

### Incident #1: README Tension Evolution Mismatch (2025-11-18)

**Symptom**: README.md contained incorrect tension evolution values in the "Key Results" summary table.

**Root Cause**: Values were manually typed into README and not synchronized when analysis code was updated during expert feedback revisions.

**Detection**: Manual inspection during publication polishing phase.

**Impact**: Publication blocker - would have caused reviewer confusion and credibility damage.

**Evidence**:
```markdown
# README.md (WRONG - before fix)
| Stage | Description | Tension vs Planck |
|-------|-------------|-------------------|
| 2 | Add uncorrelated systematics | 3.5σ |
| 3 | Add realistic correlations | 2.9σ |
| 4 | Apply three bias corrections | 1.6σ |
| 5 | Sensitivity across scenarios | 0.3σ - 1.7σ |

# create_figure1_tension_evolution.py (CORRECT - source of truth)
tension_values = [
    5.9,    # Stage 1
    4.0,    # Stage 2
    4.0,    # Stage 3
    1.9,    # Stage 4
    1.1     # Stage 5
]
```

**Discrepancy**: Stage 2-5 tensions completely wrong. README described a different analysis than implemented.

### Incident #2: Correlation Sensitivity Figure Overestimate (2025-11-18)

**Symptom**: Figures 6 & 7 (`sensitivity_correlation.png`, `figure_2d_correlation_sensitivity.png`) showed systematic budget ratios of 2.36× to 2.77×, contradicting manuscript captions claiming "1.4× (uncorrelated) and 1.6× (correlated)".

**Root Cause**: Figures were generated by an outdated script from Nov 4 (pre-v8.6H revisions). No script in current codebase regenerated these figures.

**Detection**: User manual review of figure during publication check.

**Impact**: **70% overestimate** - figures showed:
- SH0ES σ_sys: ~1.15-1.20 km/s/Mpc (should be 1.04)
- Our σ_sys: ~2.6-3.3 km/s/Mpc (should be 1.45-1.71)
- Ratio: 2.36× to 2.77× (should be 1.4× to 1.6×)

**Evidence**:
```bash
$ ls -lh figures/sensitivity_correlation.png
-rw-r--r--  1 awiley  staff   313K Nov  4 01:09 sensitivity_correlation.png

$ grep "1.4.*1.6" manuscript/manuscript.tex
caption{...systematic budget ratios of 1.4$\times$ (uncorrelated)
and 1.6$\times$ (correlated)...}
```

**Critical Failure**: Figure included in Overleaf submission package and manuscript, but values contradicted text.

### Incident #3: Figure 4 Annotation Overlap (2025-11-18)

**Symptom**: Bottom annotations in H₀ compilation forest plot overlapped (both at y=-0.8), making values unreadable.

**Root Cause**: Annotations for convergence values at x=67.48 and x=68.22 (only 0.74 km/s/Mpc apart horizontally) placed at same vertical position.

**Detection**: User visual inspection of generated figure.

**Impact**: Minor but unprofessional - degraded figure quality for publication.

**Fix**: Vertical stacking (y=-0.6 and y=-1.0) resolved overlap.

### Common Patterns Across Incidents

1. **Manual vs Automated Divergence**: Hand-edited files (README) drifted from code-generated truth
2. **Stale Generated Artifacts**: Old figure files included in package without regeneration check
3. **No Automated Validation**: All three caught by manual inspection, not automated checks
4. **High-Stakes Context**: All discovered during final polishing phase, close to submission deadline

---

## Requirements Analysis

### Functional Requirements

**FR1**: Detect discrepancies between hardcoded values in figure generation scripts and CSV data files
**FR2**: Validate README markdown tables match analysis results
**FR3**: Verify manuscript text numerical claims align with data sources
**FR4**: Check that auto-generated LaTeX tables match source CSV files
**FR5**: Ensure mathematical invariants hold (e.g., weighted means, tension calculations)
**FR6**: Confirm all figures in Overleaf package are up-to-date with current scripts

### Non-Functional Requirements

**NFR1**: **Fast execution** (<10 seconds) - must be runnable in git pre-commit hook
**NFR2**: **Clear error messages** - pinpoint exact location and nature of inconsistency
**NFR3**: **Zero false positives** - high confidence in reported errors
**NFR4**: **Minimal maintenance** - tolerate minor code structure changes without breaking
**NFR5**: **Self-documenting** - verification checks explain what they validate and why
**NFR6**: **Extensible** - easy to add new checks as manuscript evolves

### Design Constraints

**DC1**: Must work with existing codebase structure (no major refactoring required)
**DC2**: Python 3.11+ only (existing project dependency)
**DC3**: No external dependencies beyond numpy/pandas (already in environment)
**DC4**: Single-file implementation preferred for portability
**DC5**: Must produce machine-readable output (JSON) for CI/CD integration

### Success Criteria

**SC1**: Catches all three historical incidents if code is reverted
**SC2**: Zero false positives on current clean repository state
**SC3**: <5 hours total implementation + documentation time
**SC4**: Executes in <10 seconds on standard laptop
**SC5**: <500 lines of code (maintainability threshold)

---

## Architecture Design

### Core Principles

1. **Explicit over Implicit**: Hard-code expected values rather than dynamically discover them
2. **Fail Fast**: Stop at first error category to avoid noise
3. **Single Responsibility**: One verification class per data domain
4. **Tolerance Thresholds**: Allow for floating-point rounding (±0.01 for most checks)
5. **Self-Healing Hints**: Suggest fix commands when errors detected

### Component Architecture

```
verify_manuscript_consistency.py
│
├── VerificationFramework (base class)
│   ├── register_check(name, function, criticality)
│   ├── run_all_checks()
│   └── report_results(format='human'|'json')
│
├── TensionEvolutionVerifier
│   ├── verify_figure_script_values()
│   ├── verify_csv_consistency()
│   ├── verify_readme_table()
│   ├── verify_tension_calculation()
│   └── verify_manuscript_references()
│
├── SystematicBudgetVerifier
│   ├── verify_quadrature_sums()
│   ├── verify_correlation_figures()
│   ├── verify_ratio_claims()
│   └── verify_table_generation()
│
├── H0CompilationVerifier
│   ├── verify_weighted_mean_calculation()
│   ├── verify_planck_independent_convergence()
│   ├── verify_figure_annotations()
│   └── verify_readme_claims()
│
├── FigurePackageVerifier
│   ├── verify_all_figures_current()
│   ├── verify_overleaf_package_contents()
│   └── verify_regeneration_timestamps()
│
└── main()
    ├── parse_arguments()
    ├── run_verification_suite()
    └── exit_with_status_code()
```

### Data Flow

```
┌─────────────────────────────────────────────────────┐
│ 1. Load Ground Truth from Code                     │
│    - Parse figure generation scripts               │
│    - Extract hardcoded constants                   │
└─────────────────┬───────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────┐
│ 2. Load Derived Artifacts                          │
│    - CSV data files                                │
│    - Generated figures (metadata)                  │
│    - LaTeX tables                                  │
│    - README markdown                               │
└─────────────────┬───────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────┐
│ 3. Execute Verification Checks                     │
│    - Compare values with tolerance                 │
│    - Validate mathematical invariants              │
│    - Check file timestamps                         │
└─────────────────┬───────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────┐
│ 4. Aggregate Results                               │
│    - Group by severity (ERROR/WARNING/INFO)        │
│    - Associate with source files                   │
│    - Generate fix suggestions                      │
└─────────────────┬───────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────┐
│ 5. Report & Exit                                   │
│    - Human-readable summary                        │
│    - JSON for CI/CD (optional)                     │
│    - Exit code 0 (pass) or 1 (fail)               │
└─────────────────────────────────────────────────────┘
```

### Verification Check Taxonomy

**Level 1: Critical Invariants** (Block submission if violated)
- Tension calculation correctness
- Weighted mean calculations
- Quadrature sum mathematics

**Level 2: Data Consistency** (Must fix before submission)
- CSV ↔ Figure script agreement
- README ↔ Analysis results
- Table ↔ CSV data

**Level 3: Package Integrity** (Should fix, non-blocking)
- Figure file timestamps vs script modification times
- Overleaf package completeness

**Level 4: Advisory** (Good practice, optional)
- Number formatting consistency
- Significant figures alignment

---

## Implementation Plan

### Phase 1: Framework (30 minutes)

**File**: `analysis/verify_manuscript_consistency.py`

**Tasks**:
1. Create `VerificationFramework` base class with check registration
2. Implement result aggregation and reporting
3. Add JSON output for CI/CD integration
4. Write help text and usage examples

**Deliverable**: Working framework with one dummy check

### Phase 2: Critical Checks (90 minutes)

**Implement verifiers in priority order**:

1. **TensionEvolutionVerifier** (45 min)
   - Parse `create_figure1_tension_evolution.py` lines 34-56
   - Load `tension_evolution.csv`
   - Grep README.md for tension table
   - Validate all match within ±0.05σ

2. **SystematicBudgetVerifier** (30 min)
   - Verify SH0ES σ_sys = 1.04 km/s/Mpc
   - Verify Our σ_sys = 1.71 km/s/Mpc (ρ=0.3 baseline)
   - Check ratio claims: 1.4× uncorrelated, 1.6× correlated

3. **H0CompilationVerifier** (15 min)
   - Recalculate three-method weighted mean
   - Verify 67.48 ± 0.50 km/s/Mpc claim

### Phase 3: Secondary Checks (60 minutes)

4. **FigurePackageVerifier**
   - Check all figures in `scripts/prepare_overleaf_updated.sh` exist
   - Verify modification times (figure file newer than script?)

5. **TableConsistencyVerifier**
   - Cross-check table1 vs systematic_error_budget.csv

### Phase 4: Documentation (60 minutes)

**Deliverables**:
1. `docs/VERIFICATION_CHECKS.md` - Detailed check specifications
2. `docs/IMPLEMENTATION_LOG.md` - Development process narrative
3. Inline code comments explaining each check's rationale
4. README section on verification workflow

### Phase 5: Integration & Testing (30 minutes)

1. Run on current clean repository (should pass)
2. Inject deliberate errors to test detection
3. Add git pre-commit hook (optional)
4. Update `run_all.py` to call verification

**Total estimated effort**: 4.5 hours

---

## Verification Checks Specification

### Check #1: Tension Evolution Consistency

**ID**: `TENSION_EVOLUTION_001`
**Criticality**: ERROR
**Rationale**: Core result claim (5.9σ → 1.1σ)

**Source of Truth**: `analysis/create_figure1_tension_evolution.py` lines 34-56

**Artifacts to Verify**:
1. `data/tension_evolution.csv` - columns: H0_km_s_Mpc, Tension_sigma
2. `README.md` - markdown table in "Tension Reduction" section
3. `data/tables/table2_tension_evolution.tex` - LaTeX table

**Validation Logic**:
```python
# Extract from figure script
h0_values = [73.04, 73.04, 73.04, 70.54, 69.54]  # lines 34-39
sigma_values = [0.80, 1.31, 1.31, 1.65, 1.89]    # lines 42-48
tension_values = [5.9, 4.0, 4.0, 1.9, 1.1]       # lines 50-56

# Load CSV
csv_data = pd.read_csv('data/tension_evolution.csv')

# Verify each stage
for stage in range(5):
    assert abs(h0_values[stage] - csv_data.iloc[stage]['H0_km_s_Mpc']) < 0.01
    assert abs(tension_values[stage] - csv_data.iloc[stage]['Tension_sigma']) < 0.05

# Verify README contains correct values
with open('README.md') as f:
    content = f.read()
    assert '5.9σ' in content and '1.1σ' in content
```

**Example Error Message**:
```
❌ TENSION_EVOLUTION_001 FAILED
   Stage 5 tension mismatch:
   - Figure script: 1.1σ (line 56)
   - tension_evolution.csv: 1.2σ (row 5)
   - README.md: 0.3σ - 1.7σ (wrong format)

   Fix: Update README.md line 202 to match figure script value 1.1σ
```

### Check #2: Systematic Budget Ratios

**ID**: `SYSTEMATIC_BUDGET_001`
**Criticality**: ERROR
**Rationale**: Key underestimate claim (1.6× factor)

**Source of Truth**: `analysis/create_correlation_sensitivity_figures.py` lines 105-107, 247

**Expected Values**:
- SH0ES σ_sys (published): **1.04 km/s/Mpc** (constant)
- Our σ_sys (uncorrelated, ρ=0): **1.45 km/s/Mpc**
- Our σ_sys (baseline, ρ=0.3): **1.71 km/s/Mpc**
- Ratio (uncorrelated): **1.39× ≈ 1.4×**
- Ratio (correlated): **1.65× ≈ 1.6×**

**Artifacts to Verify**:
1. `figures/sensitivity_correlation.png` - must show these values
2. `figures/figure_2d_correlation_sensitivity.png` - 2D contours
3. `manuscript/manuscript.tex` - Figure 6 & 7 captions

**Validation Logic**:
```python
# Recalculate from components
SHOES_COMPONENTS = np.array([0.3, 0.0, 0.4, 0.5, 0.3, 0.4, 0.2, 0.2, 0.5])
OUR_COMPONENTS = np.array([0.3, 1.0, 0.5, 0.3, 0.3, 0.5, 0.2, 0.2, 0.5])

shoes_uncorr = np.sqrt(np.sum(SHOES_COMPONENTS**2))
our_uncorr = np.sqrt(np.sum(OUR_COMPONENTS**2))

assert abs(shoes_uncorr - 1.04) < 0.01, f"SH0ES: {shoes_uncorr:.2f} ≠ 1.04"
assert abs(our_uncorr - 1.45) < 0.01, f"Our (uncorr): {our_uncorr:.2f} ≠ 1.45"

ratio_uncorr = our_uncorr / shoes_uncorr
assert abs(ratio_uncorr - 1.39) < 0.02, f"Ratio: {ratio_uncorr:.2f} ≠ 1.39"

# Check figures exist and are recent
fig_path = Path('figures/sensitivity_correlation.png')
script_path = Path('analysis/create_correlation_sensitivity_figures.py')
assert fig_path.stat().st_mtime > script_path.stat().st_mtime, \
    "Figure older than generation script - regenerate!"
```

**Example Error Message**:
```
❌ SYSTEMATIC_BUDGET_001 FAILED
   Correlation sensitivity figure STALE:
   - sensitivity_correlation.png: modified Nov 4, 01:09
   - create_correlation_sensitivity_figures.py: modified Nov 18, 16:15

   Values in figure show 2.36× ratio (WRONG - 70% overestimate)
   Expected: 1.4× (uncorr) to 1.6× (corr)

   Fix: Run `python3 analysis/create_correlation_sensitivity_figures.py`
```

### Check #3: H₀ Compilation Convergence

**ID**: `H0_COMPILATION_001`
**Criticality**: ERROR
**Rationale**: Multi-method agreement claim

**Source of Truth**: `data/h0_measurements_compilation.csv`

**Expected Values**:
- Three-method (JAGB + CC + Planck): **67.48 ± 0.50 km/s/Mpc**
- Planck-independent (JAGB + CC): **68.22 ± 1.36 km/s/Mpc**

**Validation Logic**:
```python
data = pd.read_csv('data/h0_measurements_compilation.csv')

# Three-method convergence
subset = data[data['Method'].isin(['JAGB', 'Cosmic Chron.', 'Planck'])]
weights = 1 / subset['Sigma_km_s_Mpc']**2
weighted_mean = np.sum(subset['H0_km_s_Mpc'] * weights) / np.sum(weights)
weighted_sigma = np.sqrt(1 / np.sum(weights))

assert abs(weighted_mean - 67.48) < 0.01
assert abs(weighted_sigma - 0.50) < 0.01

# Planck-independent
subset_pi = data[data['Method'].isin(['JAGB', 'Cosmic Chron.'])]
weights_pi = 1 / subset_pi['Sigma_km_s_Mpc']**2
mean_pi = np.sum(subset_pi['H0_km_s_Mpc'] * weights_pi) / np.sum(weights_pi)

assert abs(mean_pi - 68.22) < 0.01
```

### Check #4: Figure Package Integrity

**ID**: `PACKAGE_INTEGRITY_001`
**Criticality**: WARNING
**Rationale**: Completeness check for Overleaf submission

**Expected Figures** (from `scripts/prepare_overleaf_updated.sh`):
- figure1_tension_evolution.png
- figure2_error_budget.png
- figure3_cchp_crossval_real.png
- figure4_h0_compilation.png
- figure5_h6_fit.png
- figure_correlation_heatmap.png
- sensitivity_correlation.png
- figure_2d_correlation_sensitivity.png
- corner_joint_bias_fit.png
- posterior_joint_delta_H0.png

**Validation Logic**:
```python
required_figures = [
    'figure1_tension_evolution.png',
    'figure2_error_budget.png',
    # ...
]

for fig in required_figures:
    path = Path(f'figures/{fig}')
    assert path.exists(), f"Missing figure: {fig}"

    # Check not too old (>7 days)
    age_days = (time.time() - path.stat().st_mtime) / 86400
    if age_days > 7:
        warnings.append(f"{fig} is {age_days:.0f} days old - consider regenerating")
```

---

## Future Evolution Path

### Phase 2: Single Source of Truth (Post-Submission)

If planning follow-up papers or multi-version manuscripts:

**Approach**: Extract all numerical claims to YAML config

```yaml
# config/numerical_claims.yaml
version: v8.6H

planck:
  h0: 67.36
  sigma: 0.54
  citation: "Planck Collaboration 2020"

shoes:
  h0: 73.04
  sigma_stat: 0.80
  sigma_sys: 1.04
  citation: "Riess et al. 2022"

tension_stages:
  - stage: 1
    h0: 73.04
    sigma: 0.80
    tension: 5.9
    description: "SH0ES baseline (statistical only)"
  - stage: 5
    h0: 69.54
    sigma: 1.89
    tension: 1.1
    description: "Scenario A + Prior 1 baseline"

convergence:
  three_method:
    h0: 67.48
    sigma: 0.50
    methods: [JAGB, Cosmic Chronometers, Planck]
```

**Benefits**:
- Single edit updates everywhere
- LaTeX macros auto-generated from YAML
- Zero possibility of divergence

**Effort**: ~8 hours to refactor existing codebase

### Phase 3: Property-Based Testing

For living/evolving analyses:

```python
from hypothesis import given, strategies as st

@given(
    rho_chain=st.floats(min_value=0.0, max_value=0.8),
    sigma_components=st.lists(
        st.floats(min_value=0.1, max_value=2.0),
        min_size=9,
        max_size=9
    )
)
def test_correlation_propagation_monotonic(rho_chain, sigma_components):
    """Verify σ_sys increases monotonically with ρ"""
    sigma_0 = propagate_systematics(sigma_components, rho=0.0)
    sigma_rho = propagate_systematics(sigma_components, rho=rho_chain)
    assert sigma_rho >= sigma_0
```

### Phase 4: Continuous Validation

Integrate with GitHub Actions:

```yaml
# .github/workflows/verify.yml
name: Manuscript Verification
on: [push, pull_request]
jobs:
  verify:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run verification
        run: python3 analysis/verify_manuscript_consistency.py --format=json
```

---

## Appendix A: Lessons Learned

### What Worked Well

1. **Programmatic figure generation** - Made it possible to regenerate with correct values
2. **CSV as intermediate format** - Human-readable, git-trackable, easy to verify
3. **Git version control** - Could trace when figures became stale
4. **PROVENANCE.md** - Clear documentation of data lineage helped debugging

### What Didn't Work

1. **Manual README updates** - High error rate, no validation
2. **No regeneration checks** - Old figures silently included in package
3. **Implicit contracts** - No specification of what values should match
4. **Ad-hoc validation** - Only caught errors through manual inspection

### Key Insights

1. **Verification ROI is high** - 5 hours of dev saves days of debugging
2. **Fail fast principle** - Catching errors early in pipeline is crucial
3. **Automation fragility** - Even "fully automated" pipelines can drift without validation
4. **Documentation pays off** - Well-documented code made verification possible

### Recommendations for Scientific Software

1. **Treat manuscripts like software** - Apply CI/CD principles
2. **Verify mathematical invariants** - Don't just check file equality
3. **Design for verification** - Make validation easy from the start
4. **Document numerical claims** - Maintain a "contract" of expected values
5. **Automate everything** - Including the validation itself

---

## Appendix B: Publication Notes

### Blog Post Outline

**Title**: "Verification-Driven Scientific Publishing: A Case Study in Manuscript Consistency"

**Abstract**: Describes how applying software testing principles to scientific manuscript preparation caught three critical inconsistencies just before journal submission, and presents a lightweight verification framework that other computational scientists can adapt.

**Target Venues**:
- The Overflow (Stack Overflow blog)
- Research Software Engineering Association blog
- arXiv cs.SE (Software Engineering)

**Key Messages**:
1. Scientific manuscripts have numerical invariants that can be tested
2. Manual consistency checks don't scale beyond ~20 claims
3. Verification frameworks are feasible in <5 hours
4. ROI is immediate and high for publication-ready work

### Paper Outline (Optional)

**Title**: "Contract-Driven Validation for Computational Science Manuscripts"

**Target Venue**: Computing in Science & Engineering (IEEE)

**Contributions**:
1. Taxonomy of manuscript consistency errors
2. Verification framework design patterns
3. Case study with three real incidents
4. Open-source implementation

---

**End of Design Document**
