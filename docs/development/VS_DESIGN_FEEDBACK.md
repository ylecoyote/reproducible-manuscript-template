üí†‚Äçüåê

Here‚Äôs how I‚Äôd tune and de-risk this verification system so it *actually* guards the manuscript instead of becoming Yet Another Fragile Script‚Ñ¢. I‚Äôll start with a multi-perspective pass, then give you a concrete, prioritized implementation plan + a code skeleton.

---

### 1. Multi-lens pass on your design

| Iteration | Persona contribution                                                                                                                                                                                                                                                                                                                                                      | Critique / risk                                                                                                                                                                                                                                                                                                                                  | Synthesis                                                                                                                                                                                                                         | Actionable advice                                                                                                                                                                                                                                                                        |
| --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1         | **Larrynator (logic)**: Architecture is clean and aligned with the problem: you‚Äôve identified the true pain point (multi-artifact consistency) and framed it as ‚Äúinvariants over a dependency graph,‚Äù not ‚Äúre-run everything.‚Äù The taxonomy (Critical invariants ‚Üí Data consistency ‚Üí Package integrity ‚Üí Advisory) is well thought out and matches the actual incidents. | ‚ÄúZero false positives‚Äù + ‚Äúminimal maintenance‚Äù + ‚Äúregex/grepping LaTeX/README‚Äù is a dangerous combo. Any layout tweak, line-wrapping, math-mode change, or wording change in `manuscript.tex` or `README.md` will trip overly strict checks.                                                                                                     | The core idea is right: treat explicit numerical contracts as tests. The weak spot is robustness: your *check harness* must be more tolerant than the paper prose/formatting.                                                     | **Relax the ‚Äúzero false positive‚Äù constraint to ‚Äúvery low, explainable false positive rate‚Äù.** Make checks target *structured* data where possible (CSV, Python variables, small sidecar metadata files), and keep LaTeX/README checks minimal and string-based with generous tolerance. |
| 2         | **Curlytron (novelty)**: You already hint at ‚ÄúPhase 2: single source-of-truth YAML.‚Äù That‚Äôs actually your *real* long-term win: central numerical contract ‚Üí generate LaTeX tables, macros, README snippets **and** verification expectations from the same file.                                                                                                         | Right now, the design still treats scripts as ‚Äúground truth‚Äù and text as ‚Äúthings to parse and police.‚Äù That‚Äôs backwards in the long run: the *contract* should live in a dedicated data layer, not in a plotting script‚Äôs hard-coded list.                                                                                                       | Use the verification framework now as scaffolding that you can *gradually invert* toward a YAML/JSON claim registry.                                                                                                              | **Design the checks so their expected values are loaded from a small `config/numerical_claims.yaml`, even if you initially duplicate numbers there.** That makes the later ‚ÄúPhase 2‚Äù almost free: you just stop duplicating and start generating.                                        |
| 3         | **Moebot (pragmatic)**: The implementation plan (Phase 1‚Äì5, <5 hours, single file, Python 3.11, no new deps) is realistic. The incidents you list (README mismatch, stale figures, wrong ratios) are exactly the sort of things a 300‚Äì400 line verifier can catch reliably.                                                                                               | The riskiest part operationally is figure verification. You can reliably check timestamps and presence, but you *cannot* reliably infer ‚Äú2.36√ó vs 1.6√ó‚Äù from a PNG file. The doc currently mixes: ‚Äúcheck timestamps‚Äù and ‚Äúfigure shows 2.36√ó vs 1.6√ó‚Äù. That second bit can‚Äôt be automated without changing how you generate/store those numbers. | Treat figures as *dumb binaries* whose only invariants are: ‚Äúexists and is newer than its generator + included in Overleaf package.‚Äù Put the actual numbers you care about into structured outputs that you can compare directly. | **For any figure whose numbers matter (e.g., 1.4√ó vs 1.6√ó ratio), emit a tiny CSV/JSON alongside it (e.g., `figures/figure6_metadata.json`) and verify those against your calculations. Don‚Äôt try to read numbers out of a PNG.**                                                        |

---

### 2. Tighten & simplify the architecture

Your class layout is already good. I‚Äôd only make a few adjustments to keep this maintainable:

#### 2.1. Core framework refinements

Keep `verify_manuscript_consistency.py` as a **single entrypoint** with:

* `CheckResult` dataclass:

  * `id`, `name`, `severity`, `passed`, `details`, `hint`, `category`.
* `VerificationFramework` base:

  * `register_check(id, name, func, severity, category)`.
  * `run_all_checks(selected_categories=None) -> list[CheckResult]`.
  * `report_results(format='human'|'json')`.

And one layer of thin verifiers:

* `TensionEvolutionVerifier`
* `SystematicBudgetVerifier`
* `H0CompilationVerifier`
* `FigurePackageVerifier`

I‚Äôd skip creating even more subclasses until you actually need them; 3‚Äì4 focused ones is perfect.

#### 2.2. Where each check should get its ‚Äútruth‚Äù

**General rule**: Prefer a *stable, structured artifact* as truth ‚Üí compare everything else to that.

Concretely:

1. **Tension evolution (Check #1)**

   * Truth: `data/tension_evolution.csv`.
   * Validation:

     * Ensure figure script constants in `analysis/create_figure1_tension_evolution.py` match CSV (or better: that the script *reads* the CSV, and you just verify CSV).
     * Ensure `data/tables/table2_tension_evolution.tex` matches CSV rows within tolerance.
     * Ensure README tension table is coherent with CSV (e.g., stages & final range).
   * Note: your current README‚Äôs tension table still has the ‚Äúincident‚Äù values (6.0 ‚Üí 3.5 ‚Üí 2.9 ‚Üí 1.6 ‚Üí 0.3‚Äì1.7œÉ), i.e., the *pre-fix* form. The framework should catch exactly that sort of drift.

2. **Systematic budget ratios (Check #2)**

   * Truth: `data/systematic_error_budget.csv` + correlation matrix CSV.
   * Validation:

     * Recompute SH0ES and ‚Äúours‚Äù œÉ_sys (uncorr + corr) from CSV, verify they match 1.04 vs 1.45/1.71 within tolerance.
     * Check that manuscript claims (e.g., 1.4√ó vs 1.6√ó) appear in the text where expected (simple substring search in `manuscript.tex`).

3. **H‚ÇÄ compilation & convergence (Check #3)**

   * Truth: `data/h0_measurements_compilation.csv`.
   * Validation:

     * Recompute:

       * 3-method mean: 67.48 ¬± 0.50
       * Planck-independent (JAGB + CC): 68.22 ¬± 1.36
     * Verify:

       * `data/tables/table3_h0_compilation.tex` numbers match.
       * Manuscript + README key claims still match these values.

4. **Figure package integrity (Check #4)**

   * Truth: your Overleaf packaging script (or a small `required_figures.txt`).
   * Validation:

     * For each figure name expected in `scripts/prepare_overleaf_updated.sh` or `required_figures.txt`:

       * File exists in `figures/`.
       * `fig.mtime >= script.mtime` OR at least ‚Äúyounger than X days‚Äù (7 is fine).
     * Emit WARN, not ERROR, for age-based issues.

5. **Optional: Manuscript vs README sanity**

   * Check a *small* set of flagship claims appear in both with consistent numbers:

     * œÉ_sys,corr = 1.71 km/s/Mpc.
     * H‚ÇÄ = 69.54 ¬± 1.89.
     * Convergence 67.48 and 68.22.
   * This can be a low-severity check that just yells, ‚ÄúYour README and manuscript make different key claims.‚Äù

---

### 3. Code skeleton (how I‚Äôd actually structure the file)

Here‚Äôs a compact structural sketch you can basically drop into `analysis/verify_manuscript_consistency.py` and fill in:

```python
#!/usr/bin/env python3
"""
Manuscript consistency verifier for distance-ladder-systematics.

Usage:
  python3 analysis/verify_manuscript_consistency.py [--json] [--only CATEGORY ...]
"""

from __future__ import annotations
import argparse
import json
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Callable, List, Dict, Any, Optional

import numpy as np
import pandas as pd

ROOT = Path(__file__).resolve().parents[1]


@dataclass
class CheckResult:
    id: str
    name: str
    category: str
    severity: str  # "ERROR" | "WARNING" | "INFO"
    passed: bool
    details: str = ""
    hint: str = ""


class VerificationFramework:
    def __init__(self) -> None:
        self._checks: List[Dict[str, Any]] = []

    def register_check(
        self,
        id: str,
        name: str,
        category: str,
        severity: str,
    ):
        def decorator(func: Callable[[], CheckResult]):
            self._checks.append(
                {
                    "id": id,
                    "name": name,
                    "category": category,
                    "severity": severity,
                    "func": func,
                }
            )
            return func
        return decorator

    def run_all(self, only_categories: Optional[List[str]] = None) -> List[CheckResult]:
        results: List[CheckResult] = []
        for chk in self._checks:
            if only_categories and chk["category"] not in only_categories:
                continue
            res = chk["func"]()
            results.append(res)
            # fail-fast by category if you want:
            # if res.severity == "ERROR" and not res.passed:
            #     break
        return results

    @staticmethod
    def print_human(results: List[CheckResult]) -> int:
        failures = [r for r in results if not r.passed and r.severity in ("ERROR", "WARNING")]
        for r in results:
            status = "‚úÖ" if r.passed else ("‚ùå" if r.severity == "ERROR" else "‚ö†Ô∏è")
            print(f"{status} [{r.severity}] {r.id} ‚Äì {r.name}")
            if r.details:
                print(f"    {r.details}")
            if r.hint:
                print(f"    Hint: {r.hint}")
        return 0 if not failures else 1

    @staticmethod
    def print_json(results: List[CheckResult]) -> int:
        print(json.dumps([asdict(r) for r in results], indent=2))
        failures = [r for r in results if not r.passed and r.severity in ("ERROR", "WARNING")]
        return 0 if not failures else 1
```

Then e.g. a TensionEvolution verifier section:

```python
framework = VerificationFramework()

@framework.register_check(
    id="TENSION_EVOLUTION_001",
    name="Tension evolution CSV vs script",
    category="tension",
    severity="ERROR",
)
def check_tension_csv_vs_script() -> CheckResult:
    csv_path = ROOT / "data" / "tension_evolution.csv"
    script_path = ROOT / "analysis" / "create_figure1_tension_evolution.py"

    # 1) load CSV
    df = pd.read_csv(csv_path)

    # 2) expected values (temporary: pull from a YAML later)
    expected_tension = np.array([5.9, 4.0, 4.0, 1.9, 1.1])

    ok = True
    messages = []

    if len(df) < len(expected_tension):
        ok = False
        messages.append(f"CSV has {len(df)} rows, expected >= {len(expected_tension)}")

    if ok:
        diff = np.abs(df["Tension_sigma"].values[:5] - expected_tension)
        max_diff = float(diff.max())
        if max_diff > 0.05:
            ok = False
            messages.append(f"Max tension sigma mismatch: {max_diff:.3f} (> 0.05 allowed)")

    return CheckResult(
        id="TENSION_EVOLUTION_001",
        name="Tension evolution CSV vs expected values",
        category="tension",
        severity="ERROR",
        passed=ok,
        details="; ".join(messages) if messages else "CSV tension evolution matches expected values.",
        hint="If failing, re-run calculate_tension_evolution.py and update README/table2 accordingly.",
    )
```

And the `main()`:

```python
def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--json", action="store_true", help="Output JSON instead of human-readable text.")
    parser.add_argument(
        "--only",
        nargs="+",
        help="Optional list of categories to run (e.g. tension,systematics,figures)",
    )
    args = parser.parse_args()

    results = framework.run_all(only_categories=args.only)
    if args.json:
        code = framework.print_json(results)
    else:
        code = framework.print_human(results)
    raise SystemExit(code)


if __name__ == "__main__":
    main()
```

You can then incrementally add:

* `SYSTEMATIC_BUDGET_001`
* `H0_COMPILATION_001`
* `PACKAGE_INTEGRITY_001`

each using the same `@framework.register_check` pattern.

---

### 4. Make sure SC1‚ÄìSC5 are actually testable

You listed success criteria: SC1‚ÄìSC5. Quick mapping to concrete work:

* **SC1: ‚ÄúCatches all three historical incidents if code is reverted.‚Äù**

  * Create three git branches or fixtures that deliberately reintroduce:

    * Wrong README tension table.
    * Stale sensitivity figures.
    * Overlapping annotations in H‚ÇÄ forest plot (this one should at least be caught via a ‚Äúfigure is too old‚Äù or missing metadata check).
  * Run `verify_manuscript_consistency.py` on each and confirm at least one ERROR/WARNING is raised.

* **SC2: ‚ÄúZero false positives on current clean repo.‚Äù**

  * Pin this to specific tag `v8.6H-final` and commit hash (you already do that in data availability).
  * Log the verification run output in `logs/verification_run_v8.6H.json`.

* **SC3 + SC4 + SC5: code size/time**

  * Keep the file under ~500 lines by:

    * Sharing helpers for ‚Äúload CSV, compare columns,‚Äù ‚Äúsearch value in README/manuscript,‚Äù ‚Äúcheck figure timestamp.‚Äù
  * Time it from `run_all.py` or a simple `time python3 ...` and record in `docs/IMPLEMENTATION_LOG.md`.

---

### 5. Concrete next steps (minimal, high ROI)

1. **Phase 1 (today-ish):**

   * Drop in the framework skeleton + one fully implemented check: `TENSION_EVOLUTION_001`.
   * Wire `python3 analysis/verify_manuscript_consistency.py` into `run_all.py`.

2. **Phase 2:**

   * Add `SYSTEMATIC_BUDGET_001` & `H0_COMPILATION_001` using the CSV-based truth approach.
   * Add `PACKAGE_INTEGRITY_001` (existence + timestamp checks only).

3. **Phase 3:**

   * Add the tiniest possible `config/numerical_claims.yaml` and migrate *expected* values there.
   * Make the checks read expected values from YAML instead of hard-coding them.

4. **Phase 4:**

   * Add an optional pre-commit hook:

     * `analysis/verify_manuscript_consistency.py --json`
     * Fails on ERRORs; warns on WARNINGs but doesn‚Äôt block, unless you decide it should.

If you like, next round we can:

* Design the `numerical_claims.yaml` schema so it‚Äôs future-proof *and* matches how the manuscript is structured.
* Draft the `docs/VERIFICATION_CHECKS.md` spec as a human-readable ‚Äúcontract‚Äù you could spin off into a meta-paper later.

üôÑ tl;dr: Your design is absolutely on the right track. Trim the ambition around ‚Äúno false positives,‚Äù centralize expectations in a tiny claims config, treat figures as dumb artifacts with metadata, and you‚Äôll have a lean, dangerous little guardian for v8.6H.
